/*
 * Copyright (c) Microsoft Corporation.
 * Licensed under the MIT License.
 *
 * Code generated by Microsoft (R) AutoRest Code Generator.
 * Changes may cause incorrect behavior and will be lost if the code is regenerated.
 */

import * as coreHttp from "@azure/core-http";

/**
 * Contains the list of all operations supported by BatchAI resource provider
 */
export interface OperationListResult {
  /**
   * The list of operations supported by the resource provider.
   */
  readonly value?: Operation[];
  /**
   * The URL to get the next set of operation list results if there are any.
   */
  readonly nextLink?: string;
}

/**
 * Details of a REST API operation
 */
export interface Operation {
  /**
   * This is of the format {provider}/{resource}/{operation}
   */
  readonly name?: string;
  /**
   * The object that describes the operation.
   */
  display?: OperationDisplay;
  /**
   * The intended executor of the operation.
   */
  readonly origin?: string;
  /**
   * Any object
   */
  properties?: any;
}

/**
 * The object that describes the operation.
 */
export interface OperationDisplay {
  /**
   * Friendly name of the resource provider.
   */
  readonly provider?: string;
  /**
   * For example: read, write, delete, or listKeys/action
   */
  readonly operation?: string;
  /**
   * The resource type on which the operation is performed.
   */
  readonly resource?: string;
  /**
   * The friendly name of the operation.
   */
  readonly description?: string;
}

/**
 * The List Usages operation response.
 */
export interface ListUsagesResult {
  /**
   * The list of compute resource usages.
   */
  readonly value?: Usage[];
  /**
   * The URI to fetch the next page of compute resource usage information. Call ListNext() with this to fetch the next page of compute resource usage information.
   */
  readonly nextLink?: string;
}

/**
 * Describes Batch AI Resource Usage.
 */
export interface Usage {
  /**
   * An enum describing the unit of usage measurement.
   */
  readonly unit?: "Count";
  /**
   * The current usage of the resource.
   */
  readonly currentValue?: number;
  /**
   * The maximum permitted usage of the resource.
   */
  readonly limit?: number;
  /**
   * The name of the type of usage.
   */
  readonly name?: UsageName;
}

/**
 * The Usage Names.
 */
export interface UsageName {
  /**
   * The name of the resource.
   */
  readonly value?: string;
  /**
   * The localized name of the resource.
   */
  readonly localizedValue?: string;
}

/**
 * Values returned by the List operation.
 */
export interface WorkspaceListResult {
  /**
   * The collection of workspaces.
   */
  readonly value?: Workspace[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * A definition of an Azure resource.
 */
export interface Resource {
  /**
   * The ID of the resource
   */
  readonly id?: string;
  /**
   * The name of the resource
   */
  readonly name?: string;
  /**
   * The type of the resource
   */
  readonly typeModel?: string;
  /**
   * The location of the resource
   */
  readonly location?: string;
  /**
   * The tags of the resource
   */
  readonly tags?: { [propertyName: string]: string };
}

/**
 * Batch AI Workspace information.
 */
export type Workspace = Resource & {
  /**
   * Time when the Workspace was created.
   */
  readonly creationTime?: Date;
  /**
   * The provisioned state of the Workspace
   */
  readonly provisioningState?: ProvisioningState;
  /**
   * The time at which the workspace entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
};

/**
 * An error response from the Batch AI service.
 */
export interface CloudError {
  /**
   * An error response from the Batch AI service.
   */
  readonly error?: CloudErrorBody;
}

/**
 * An error response from the Batch AI service.
 */
export interface CloudErrorBody {
  /**
   * An identifier for the error. Codes are invariant and are intended to be consumed programmatically.
   */
  readonly code?: string;
  /**
   * A message describing the error, intended to be suitable for display in a user interface.
   */
  readonly message?: string;
  /**
   * The target of the particular error. For example, the name of the property in error.
   */
  readonly target?: string;
  /**
   * A list of additional details about the error.
   */
  readonly details?: CloudErrorBody[];
}

/**
 * Workspace creation parameters.
 */
export interface WorkspaceCreateParameters {
  /**
   * The region in which to create the Workspace.
   */
  location: string;
  /**
   * The user specified tags associated with the Workspace.
   */
  tags?: { [propertyName: string]: string };
}

/**
 * Workspace update parameters.
 */
export interface WorkspaceUpdateParameters {
  /**
   * The user specified tags associated with the Workspace.
   */
  tags?: { [propertyName: string]: string };
}

/**
 * Values returned by the List operation.
 */
export interface ExperimentListResult {
  /**
   * The collection of experiments.
   */
  readonly value?: Experiment[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * A definition of an Azure proxy resource.
 */
export interface ProxyResource {
  /**
   * The ID of the resource.
   */
  readonly id?: string;
  /**
   * The name of the resource.
   */
  readonly name?: string;
  /**
   * The type of the resource.
   */
  readonly typeModel?: string;
}

/**
 * Experiment information.
 */
export type Experiment = ProxyResource & {
  /**
   * Time when the Experiment was created.
   */
  readonly creationTime?: Date;
  /**
   * The provisioned state of the experiment
   */
  readonly provisioningState?: ProvisioningState;
  /**
   * The time at which the experiment entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
};

/**
 * Values returned by the List operation.
 */
export interface JobListResult {
  /**
   * The collection of jobs.
   */
  readonly value?: Job[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Information about a Job.
 */
export type Job = ProxyResource & {
  /**
   * Scheduling priority associated with the job.
   */
  schedulingPriority?: JobPriority;
  /**
   * Resource ID of the cluster associated with the job.
   */
  cluster?: ResourceId;
  /**
   * Collection of mount volumes available to the job during execution. These volumes are mounted before the job execution and unmounted after the job completion. The volumes are mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * The job will be gang scheduled on that many compute nodes
   */
  nodeCount?: number;
  /**
   * If the container was downloaded as part of cluster setup then the same container image will be used. If not provided, the job will run on the VM.
   */
  containerSettings?: ContainerSettings;
  /**
   * Possible values are: cntk, tensorflow, caffe, caffe2, chainer, pytorch, custom, custommpi, horovod.
   */
  toolType?: ToolType;
  /**
   * CNTK (aka Microsoft Cognitive Toolkit) job settings.
   */
  cntkSettings?: CNTKsettings;
  /**
   * pyTorch job settings.
   */
  pyTorchSettings?: PyTorchSettings;
  /**
   * TensorFlow job settings.
   */
  tensorFlowSettings?: TensorFlowSettings;
  /**
   * Caffe job settings.
   */
  caffeSettings?: CaffeSettings;
  /**
   * Caffe2 job settings.
   */
  caffe2Settings?: Caffe2Settings;
  /**
   * Chainer job settings.
   */
  chainerSettings?: ChainerSettings;
  /**
   * Custom tool kit job settings.
   */
  customToolkitSettings?: CustomToolkitSettings;
  /**
   * Custom MPI job settings.
   */
  customMpiSettings?: CustomMpiSettings;
  /**
   * Specifies the settings for Horovod job.
   */
  horovodSettings?: HorovodSettings;
  /**
   * The specified actions will run on all the nodes that are part of the job
   */
  jobPreparation?: JobPreparation;
  /**
   * A segment of job's output directories path created by Batch AI. Batch AI creates job's output directories under an unique path to avoid conflicts between jobs. This value contains a path segment generated by Batch AI to make the path unique and can be used to find the output directory on the node or mounted filesystem.
   */
  readonly jobOutputDirectoryPathSegment?: string;
  /**
   * The path where the Batch AI service stores stdout, stderror and execution log of the job.
   */
  stdOutErrPathPrefix?: string;
  /**
   * A list of input directories for the job.
   */
  inputDirectories?: InputDirectory[];
  /**
   * A list of output directories for the job.
   */
  outputDirectories?: OutputDirectory[];
  /**
   * A collection of user defined environment variables to be setup for the job.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * A collection of user defined environment variables with secret values to be setup for the job. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * Constraints associated with the Job.
   */
  constraints?: JobPropertiesConstraints;
  /**
   * The creation time of the job.
   */
  readonly creationTime?: Date;
  /**
   * The provisioned state of the Batch AI job
   */
  readonly provisioningState?: ProvisioningState;
  /**
   * The time at which the job entered its current provisioning state.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * The current state of the job. Possible values are: queued - The job is queued and able to run. A job enters this state when it is created, or when it is awaiting a retry after a failed run. running - The job is running on a compute cluster. This includes job-level preparation such as downloading resource files or set up container specified on the job - it does not necessarily mean that the job command line has started executing. terminating - The job is terminated by the user, the terminate operation is in progress. succeeded - The job has completed running successfully and exited with exit code 0. failed - The job has finished unsuccessfully (failed with a non-zero exit code) and has exhausted its retry limit. A job is also marked as failed if an error occurred launching the job.
   */
  readonly executionState?: ExecutionState;
  /**
   * The time at which the job entered its current execution state.
   */
  readonly executionStateTransitionTime?: Date;
  /**
   * Information about the execution of a job.
   */
  executionInfo?: JobPropertiesExecutionInfo;
};

/**
 * Represents a resource ID. For example, for a subnet, it is the resource URL for the subnet.
 */
export interface ResourceId {
  /**
   * The ID of the resource
   */
  id: string;
}

/**
 * Details of volumes to mount on the cluster.
 */
export interface MountVolumes {
  /**
   * A collection of Azure File Shares that are to be mounted to the cluster nodes.
   */
  azureFileShares?: AzureFileShareReference[];
  /**
   * A collection of Azure Blob Containers that are to be mounted to the cluster nodes.
   */
  azureBlobFileSystems?: AzureBlobFileSystemReference[];
  /**
   * A collection of Batch AI File Servers that are to be mounted to the cluster nodes.
   */
  fileServers?: FileServerReference[];
  /**
   * A collection of unmanaged file systems that are to be mounted to the cluster nodes.
   */
  unmanagedFileSystems?: UnmanagedFileSystemReference[];
}

/**
 * Azure File Share mounting configuration.
 */
export interface AzureFileShareReference {
  /**
   * Name of the Azure storage account.
   */
  accountName: string;
  /**
   * URL to access the Azure File.
   */
  azureFileUrl: string;
  /**
   * Information about the Azure storage credentials.
   */
  credentials: AzureStorageCredentialsInfo;
  /**
   * The relative path on the compute node where the Azure File share will be mounted. Note that all cluster level file shares will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file shares will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * File mode for files on the mounted file share. Default value: 0777.
   */
  fileMode?: string;
  /**
   * File mode for directories on the mounted file share. Default value: 0777.
   */
  directoryMode?: string;
}

/**
 * Azure storage account credentials.
 */
export interface AzureStorageCredentialsInfo {
  /**
   * Storage account key. One of accountKey or accountKeySecretReference must be specified.
   */
  accountKey?: string;
  /**
   * Information about KeyVault secret storing the storage account key. One of accountKey or accountKeySecretReference must be specified.
   */
  accountKeySecretReference?: KeyVaultSecretReference;
}

/**
 * Key Vault Secret reference.
 */
export interface KeyVaultSecretReference {
  /**
   * Fully qualified resource identifier of the Key Vault.
   */
  sourceVault: ResourceId;
  /**
   * The URL referencing a secret in the Key Vault.
   */
  secretUrl: string;
}

/**
 * Azure Blob Storage Container mounting configuration.
 */
export interface AzureBlobFileSystemReference {
  /**
   * Name of the Azure storage account.
   */
  accountName: string;
  /**
   * Name of the Azure Blob Storage container to mount on the cluster.
   */
  containerName: string;
  /**
   * Information about the Azure storage credentials.
   */
  credentials: AzureStorageCredentialsInfo;
  /**
   * The relative path on the compute node where the Azure File container will be mounted. Note that all cluster level containers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level containers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * Mount options for mounting blobfuse file system.
   */
  mountOptions?: string;
}

/**
 * File Server mounting configuration.
 */
export interface FileServerReference {
  /**
   * Resource ID of the existing File Server to be mounted.
   */
  fileServer: ResourceId;
  /**
   * File Server directory that needs to be mounted. If this property is not specified, the entire File Server will be mounted.
   */
  sourceDirectory?: string;
  /**
   * The relative path on the compute node where the File Server will be mounted. Note that all cluster level file servers will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level file servers will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
  /**
   * Mount options to be passed to mount command.
   */
  mountOptions?: string;
}

/**
 * Unmanaged file system mounting configuration.
 */
export interface UnmanagedFileSystemReference {
  /**
   * Mount command line. Note, Batch AI will append mount path to the command on its own.
   */
  mountCommand: string;
  /**
   * The relative path on the compute node where the unmanaged file system will be mounted. Note that all cluster level unmanaged file systems will be mounted under $AZ_BATCHAI_MOUNT_ROOT location and all job level unmanaged file systems will be mounted under $AZ_BATCHAI_JOB_MOUNT_ROOT.
   */
  relativeMountPath: string;
}

/**
 * Docker container settings.
 */
export interface ContainerSettings {
  /**
   * Information about docker image and docker registry to download the container from.
   */
  imageSourceRegistry: ImageSourceRegistry;
  /**
   * Size of /dev/shm. Please refer to docker documentation for supported argument formats.
   */
  shmSize?: string;
}

/**
 * Information about docker image for the job.
 */
export interface ImageSourceRegistry {
  /**
   * URL for image repository.
   */
  serverUrl?: string;
  /**
   * The name of the image in the image repository.
   */
  image: string;
  /**
   * Credentials to access the private docker repository.
   */
  credentials?: PrivateRegistryCredentials;
}

/**
 * Credentials to access a container image in a private repository.
 */
export interface PrivateRegistryCredentials {
  /**
   * User name to login to the repository.
   */
  username: string;
  /**
   * User password to login to the docker repository. One of password or passwordSecretReference must be specified.
   */
  password?: string;
  /**
   * KeyVault Secret storing the password. Users can store their secrets in Azure KeyVault and pass it to the Batch AI service to integrate with KeyVault. One of password or passwordSecretReference must be specified.
   */
  passwordSecretReference?: KeyVaultSecretReference;
}

/**
 * CNTK (aka Microsoft Cognitive Toolkit) job settings.
 */
export interface CNTKsettings {
  /**
   * The language to use for launching CNTK (aka Microsoft Cognitive Toolkit) job. Valid values are 'BrainScript' or 'Python'.
   */
  languageType?: string;
  /**
   * Specifies the path of the BrainScript config file. This property can be specified only if the languageType is 'BrainScript'.
   */
  configFilePath?: string;
  /**
   * Python script to execute. This property can be specified only if the languageType is 'Python'.
   */
  pythonScriptFilePath?: string;
  /**
   * The path to the Python interpreter. This property can be specified only if the languageType is 'Python'.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script or cntk executable.
   */
  commandLineArgs?: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * pyTorch job settings.
 */
export interface PyTorchSettings {
  /**
   * The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
  /**
   * Type of the communication backend for distributed jobs. Valid values are 'TCP', 'Gloo' or 'MPI'. Not required for non-distributed jobs.
   */
  communicationBackend?: string;
}

/**
 * TensorFlow job settings.
 */
export interface TensorFlowSettings {
  /**
   * The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script for the master task.
   */
  masterCommandLineArgs?: string;
  /**
   * Command line arguments that need to be passed to the python script for the worker task. Optional for single process jobs.
   */
  workerCommandLineArgs?: string;
  /**
   * Command line arguments that need to be passed to the python script for the parameter server. Optional for single process jobs.
   */
  parameterServerCommandLineArgs?: string;
  /**
   * The number of worker tasks. If specified, the value must be less than or equal to (nodeCount * numberOfGPUs per VM). If not specified, the default value is equal to nodeCount. This property can be specified only for distributed TensorFlow training.
   */
  workerCount?: number;
  /**
   * The number of parameter server tasks. If specified, the value must be less than or equal to nodeCount. If not specified, the default value is equal to 1 for distributed TensorFlow training. This property can be specified only for distributed TensorFlow training.
   */
  parameterServerCount?: number;
}

/**
 * Caffe job settings.
 */
export interface CaffeSettings {
  /**
   * Path of the config file for the job. This property cannot be specified if pythonScriptFilePath is specified.
   */
  configFilePath?: string;
  /**
   * Python script to execute. This property cannot be specified if configFilePath is specified.
   */
  pythonScriptFilePath?: string;
  /**
   * The path to the Python interpreter. The property can be specified only if the pythonScriptFilePath is specified.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the Caffe job.
   */
  commandLineArgs?: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Caffe2 job settings.
 */
export interface Caffe2Settings {
  /**
   * The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
}

/**
 * Chainer job settings.
 */
export interface ChainerSettings {
  /**
   * The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Custom tool kit job settings.
 */
export interface CustomToolkitSettings {
  /**
   * The command line to execute on the master node.
   */
  commandLine?: string;
}

/**
 * Custom MPI job settings.
 */
export interface CustomMpiSettings {
  /**
   * The command line to be executed by mpi runtime on each compute node.
   */
  commandLine: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Specifies the settings for Horovod job.
 */
export interface HorovodSettings {
  /**
   * The python script to execute.
   */
  pythonScriptFilePath: string;
  /**
   * The path to the Python interpreter.
   */
  pythonInterpreterPath?: string;
  /**
   * Command line arguments that need to be passed to the python script.
   */
  commandLineArgs?: string;
  /**
   * Number of processes to launch for the job execution. The default value for this property is equal to nodeCount property
   */
  processCount?: number;
}

/**
 * Job preparation settings.
 */
export interface JobPreparation {
  /**
   * The command line to execute. If containerSettings is specified on the job, this commandLine will be executed in the same container as job. Otherwise it will be executed on the node.
   */
  commandLine: string;
}

/**
 * Input directory for the job.
 */
export interface InputDirectory {
  /**
   * The ID for the input directory. The job can use AZ_BATCHAI_INPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
   */
  id: string;
  /**
   * The path to the input directory.
   */
  path: string;
}

/**
 * Output directory for the job.
 */
export interface OutputDirectory {
  /**
   * The ID of the output directory. The job can use AZ_BATCHAI_OUTPUT_<id> environment variable to find the directory path, where <id> is the value of id attribute.
   */
  id: string;
  /**
   * The prefix path where the output directory will be created. Note, this is an absolute path to prefix. E.g. $AZ_BATCHAI_MOUNT_ROOT/MyNFS/MyLogs. The full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.
   */
  pathPrefix: string;
  /**
   * The suffix path where the output directory will be created. E.g. models. You can find the full path to the output directory by combining pathPrefix, jobOutputDirectoryPathSegment (reported by get job) and pathSuffix.
   */
  pathSuffix?: string;
}

/**
 * An environment variable definition.
 */
export interface EnvironmentVariable {
  /**
   * The name of the environment variable.
   */
  name: string;
  /**
   * The value of the environment variable.
   */
  value: string;
}

/**
 * An environment variable with secret value definition.
 */
export interface EnvironmentVariableWithSecretValue {
  /**
   * The name of the environment variable to store the secret value.
   */
  name: string;
  /**
   * The value of the environment variable. This value will never be reported back by Batch AI.
   */
  value?: string;
  /**
   * KeyVault store and secret which contains the value for the environment variable. One of value or valueSecretReference must be provided.
   */
  valueSecretReference?: KeyVaultSecretReference;
}

/**
 * Constraints associated with the Job.
 */
export interface JobPropertiesConstraints {
  /**
   * Max time the job can run. Default value: 1 week.
   */
  maxWallClockTime?: string;
}

/**
 * Information about the execution of a job.
 */
export interface JobPropertiesExecutionInfo {
  /**
   * The time at which the job started running. 'Running' corresponds to the running state. If the job has been restarted or retried, this is the most recent time at which the job started running. This property is present only for job that are in the running or completed state.
   */
  readonly startTime?: Date;
  /**
   * The time at which the job completed. This property is only returned if the job is in completed state.
   */
  readonly endTime?: Date;
  /**
   * The exit code of the job. This property is only returned if the job is in completed state.
   */
  readonly exitCode?: number;
  /**
   * A collection of errors encountered by the service during job execution.
   */
  readonly errors?: BatchAIError[];
}

/**
 * An error response from the Batch AI service.
 */
export interface BatchAIError {
  /**
   * An identifier of the error. Codes are invariant and are intended to be consumed programmatically.
   */
  readonly code?: string;
  /**
   * A message describing the error, intended to be suitable for display in a user interface.
   */
  readonly message?: string;
  /**
   * A list of additional details about the error.
   */
  readonly details?: NameValuePair[];
}

/**
 * Name-value pair.
 */
export interface NameValuePair {
  /**
   * The name in the name-value pair.
   */
  name?: string;
  /**
   * The value in the name-value pair.
   */
  value?: string;
}

/**
 * Job creation parameters.
 */
export interface JobCreateParameters {
  /**
   * Scheduling priority associated with the job. Possible values: low, normal, high.
   */
  schedulingPriority?: JobPriority;
  /**
   * Resource ID of the cluster on which this job will run.
   */
  cluster?: ResourceId;
  /**
   * Information on mount volumes to be used by the job. These volumes will be mounted before the job execution and will be unmounted after the job completion. The volumes will be mounted at location specified by $AZ_BATCHAI_JOB_MOUNT_ROOT environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * Number of compute nodes to run the job on. The job will be gang scheduled on that many compute nodes.
   */
  nodeCount?: number;
  /**
   * Docker container settings for the job. If not provided, the job will run directly on the node.
   */
  containerSettings?: ContainerSettings;
  /**
   * Settings for CNTK (aka Microsoft Cognitive Toolkit) job.
   */
  cntkSettings?: CNTKsettings;
  /**
   * Settings for pyTorch job.
   */
  pyTorchSettings?: PyTorchSettings;
  /**
   * Settings for Tensor Flow job.
   */
  tensorFlowSettings?: TensorFlowSettings;
  /**
   * Settings for Caffe job.
   */
  caffeSettings?: CaffeSettings;
  /**
   * Settings for Caffe2 job.
   */
  caffe2Settings?: Caffe2Settings;
  /**
   * Settings for Chainer job.
   */
  chainerSettings?: ChainerSettings;
  /**
   * Settings for custom tool kit job.
   */
  customToolkitSettings?: CustomToolkitSettings;
  /**
   * Settings for custom MPI job.
   */
  customMpiSettings?: CustomMpiSettings;
  /**
   * Settings for Horovod job.
   */
  horovodSettings?: HorovodSettings;
  /**
   * A command line to be executed on each node allocated for the job before tool kit is launched.
   */
  jobPreparation?: JobPreparation;
  /**
   * The path where the Batch AI service will store stdout, stderror and execution log of the job.
   */
  stdOutErrPathPrefix?: string;
  /**
   * A list of input directories for the job.
   */
  inputDirectories?: InputDirectory[];
  /**
   * A list of output directories for the job.
   */
  outputDirectories?: OutputDirectory[];
  /**
   * A list of user defined environment variables which will be setup for the job.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * A list of user defined environment variables with secret values which will be setup for the job. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * Constraints associated with the Job.
   */
  constraints?: JobBasePropertiesConstraints;
}

/**
 * Constraints associated with the Job.
 */
export interface JobBasePropertiesConstraints {
  /**
   * Max time the job can run. Default value: 1 week.
   */
  maxWallClockTime?: string;
}

/**
 * Values returned by the List operation.
 */
export interface FileListResult {
  /**
   * The collection of returned job directories and files.
   */
  readonly value?: File[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Properties of the file or directory.
 */
export interface File {
  /**
   * Name of the file.
   */
  readonly name?: string;
  /**
   * Type of the file. Possible values are file and directory.
   */
  readonly fileType?: FileType;
  /**
   * URL to download the corresponding file. The downloadUrl is not returned for directories.
   */
  readonly downloadUrl?: string;
  /**
   * The time at which the file was last modified.
   */
  readonly lastModified?: Date;
  /**
   * The file of the size.
   */
  readonly contentLength?: number;
}

/**
 * Values returned by the List operation.
 */
export interface RemoteLoginInformationListResult {
  /**
   * The collection of returned remote login details.
   */
  readonly value?: RemoteLoginInformation[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Login details to SSH to a compute node in cluster.
 */
export interface RemoteLoginInformation {
  /**
   * ID of the compute node.
   */
  readonly nodeId?: string;
  /**
   * Public IP address of the compute node.
   */
  readonly ipAddress?: string;
  /**
   * SSH port number of the node.
   */
  readonly port?: number;
}

/**
 * File Server creation parameters.
 */
export interface FileServerCreateParameters {
  /**
   * The size of the virtual machine for the File Server. For information about available VM sizes from the Virtual Machines Marketplace, see Sizes for Virtual Machines (Linux).
   */
  vmSize?: string;
  /**
   * SSH configuration for the File Server node.
   */
  sshConfiguration?: SshConfiguration;
  /**
   * Settings for the data disks which will be created for the File Server.
   */
  dataDisks?: DataDisks;
  /**
   * Identifier of an existing virtual network subnet to put the File Server in. If not provided, a new virtual network and subnet will be created.
   */
  subnet?: ResourceId;
}

/**
 * SSH configuration.
 */
export interface SshConfiguration {
  /**
   * List of source IP ranges to allow SSH connection from. The default value is '*' (all source IPs are allowed). Maximum number of IP ranges that can be specified is 400.
   */
  publicIPsToAllow?: string[];
  /**
   * Settings for administrator user account to be created on a node. The account can be used to establish SSH connection to the node.
   */
  userAccountSettings: UserAccountSettings;
}

/**
 * Settings for user account that gets created on each on the nodes of a cluster.
 */
export interface UserAccountSettings {
  /**
   * Name of the administrator user account which can be used to SSH to nodes.
   */
  adminUserName: string;
  /**
   * SSH public key of the administrator user account.
   */
  adminUserSshPublicKey?: string;
  /**
   * Password of the administrator user account.
   */
  adminUserPassword?: string;
}

/**
 * Data disks settings.
 */
export interface DataDisks {
  /**
   * Disk size in GB for the blank data disks.
   */
  diskSizeInGB: number;
  /**
   * Caching type for the disks. Available values are none (default), readonly, readwrite. Caching type can be set only for VM sizes supporting premium storage.
   */
  cachingType?: CachingType;
  /**
   * Number of data disks attached to the File Server. If multiple disks attached, they will be configured in RAID level 0.
   */
  diskCount: number;
  /**
   * Type of storage account to be used on the disk. Possible values are: Standard_LRS or Premium_LRS. Premium storage account type can only be used with VM sizes supporting premium storage.
   */
  storageAccountType: StorageAccountType;
}

/**
 * File Server information.
 */
export type FileServer = ProxyResource & {
  /**
   * VM size of the File Server.
   */
  vmSize?: string;
  /**
   * SSH configuration for accessing the File Server node.
   */
  sshConfiguration?: SshConfiguration;
  /**
   * Information about disks attached to File Server VM.
   */
  dataDisks?: DataDisks;
  /**
   * File Server virtual network subnet resource ID.
   */
  subnet?: ResourceId;
  /**
   * File Server mount settings.
   */
  readonly mountSettings?: MountSettings;
  /**
   * Time when the provisioning state was changed.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * Time when the FileServer was created.
   */
  readonly creationTime?: Date;
  /**
   * Provisioning state of the File Server. Possible values: creating - The File Server is getting created; updating - The File Server creation has been accepted and it is getting updated; deleting - The user has requested that the File Server be deleted, and it is in the process of being deleted; failed - The File Server creation has failed with the specified error code. Details about the error code are specified in the message field; succeeded - The File Server creation has succeeded.
   */
  readonly provisioningState?: FileServerProvisioningState;
};

/**
 * File Server mount Information.
 */
export interface MountSettings {
  /**
   * Path where the data disks are mounted on the File Server.
   */
  mountPoint?: string;
  /**
   * Public IP address of the File Server which can be used to SSH to the node from outside of the subnet.
   */
  fileServerPublicIP?: string;
  /**
   * Internal IP address of the File Server which can be used to access the File Server from within the subnet.
   */
  fileServerInternalIP?: string;
}

/**
 * Values returned by the File Server List operation.
 */
export interface FileServerListResult {
  /**
   * The collection of File Servers.
   */
  value?: FileServer[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Cluster creation operation.
 */
export interface ClusterCreateParameters {
  /**
   * The size of the virtual machines in the cluster. All nodes in a cluster have the same VM size. For information about available VM sizes for clusters using images from the Virtual Machines Marketplace see Sizes for Virtual Machines (Linux). Batch AI service supports all Azure VM sizes except STANDARD_A0 and those with premium storage (STANDARD_GS, STANDARD_DS, and STANDARD_DSV2 series).
   */
  vmSize?: string;
  /**
   * VM priority. Allowed values are: dedicated (default) and lowpriority.
   */
  vmPriority?: VmPriority;
  /**
   * Scale settings for the cluster. Batch AI service supports manual and auto scale clusters.
   */
  scaleSettings?: ScaleSettings;
  /**
   * OS image configuration for cluster nodes. All nodes in a cluster have the same OS image.
   */
  virtualMachineConfiguration?: VirtualMachineConfiguration;
  /**
   * Setup to be performed on each compute node in the cluster.
   */
  nodeSetup?: NodeSetup;
  /**
   * Settings for an administrator user account that will be created on each compute node in the cluster.
   */
  userAccountSettings?: UserAccountSettings;
  /**
   * Existing virtual network subnet to put the cluster nodes in. Note, if a File Server mount configured in node setup, the File Server's subnet will be used automatically.
   */
  subnet?: ResourceId;
}

/**
 * At least one of manual or autoScale settings must be specified. Only one of manual or autoScale settings can be specified. If autoScale settings are specified, the system automatically scales the cluster up and down (within the supplied limits) based on the pending jobs on the cluster.
 */
export interface ScaleSettings {
  /**
   * Manual scale settings for the cluster.
   */
  manual?: ManualScaleSettings;
  /**
   * Auto-scale settings for the cluster.
   */
  autoScale?: AutoScaleSettings;
}

/**
 * Manual scale settings for the cluster.
 */
export interface ManualScaleSettings {
  /**
   * The desired number of compute nodes in the Cluster. Default is 0.
   */
  targetNodeCount: number;
  /**
   * An action to be performed when the cluster size is decreasing. The default value is requeue.
   */
  nodeDeallocationOption?: DeallocationOption;
}

/**
 * Auto-scale settings for the cluster. The system automatically scales the cluster up and down (within minimumNodeCount and maximumNodeCount) based on the number of queued and running jobs assigned to the cluster.
 */
export interface AutoScaleSettings {
  /**
   * The minimum number of compute nodes the Batch AI service will try to allocate for the cluster. Note, the actual number of nodes can be less than the specified value if the subscription has not enough quota to fulfill the request.
   */
  minimumNodeCount: number;
  /**
   * The maximum number of compute nodes the cluster can have.
   */
  maximumNodeCount: number;
  /**
   * The number of compute nodes to allocate on cluster creation. Note that this value is used only during cluster creation. Default: 0.
   */
  initialNodeCount?: number;
}

/**
 * VM configuration.
 */
export interface VirtualMachineConfiguration {
  /**
   * OS image reference for cluster nodes.
   */
  imageReference?: ImageReference;
}

/**
 * The OS image reference.
 */
export interface ImageReference {
  /**
   * Publisher of the image.
   */
  publisher: string;
  /**
   * Offer of the image.
   */
  offer: string;
  /**
   * SKU of the image.
   */
  sku: string;
  /**
   * Version of the image.
   */
  version?: string;
  /**
   * The ARM resource identifier of the virtual machine image for the compute nodes. This is of the form /subscriptions/{subscriptionId}/resourceGroups/{resourceGroup}/providers/Microsoft.Compute/images/{imageName}. The virtual machine image must be in the same region and subscription as the cluster. For information about the firewall settings for the Batch node agent to communicate with the Batch service see https://docs.microsoft.com/en-us/azure/batch/batch-api-basics#virtual-network-vnet-and-firewall-configuration. Note, you need to provide publisher, offer and sku of the base OS image of which the custom image has been derived from.
   */
  virtualMachineImageId?: string;
}

/**
 * Node setup settings.
 */
export interface NodeSetup {
  /**
   * Setup task to run on cluster nodes when nodes got created or rebooted. The setup task code needs to be idempotent. Generally the setup task is used to download static data that is required for all jobs that run on the cluster VMs and/or to download/install software.
   */
  setupTask?: SetupTask;
  /**
   * Mount volumes to be available to setup task and all jobs executing on the cluster. The volumes will be mounted at location specified by $AZ_BATCHAI_MOUNT_ROOT environment variable.
   */
  mountVolumes?: MountVolumes;
  /**
   * Settings for performance counters collecting and uploading.
   */
  performanceCountersSettings?: PerformanceCountersSettings;
}

/**
 * Specifies a setup task which can be used to customize the compute nodes of the cluster.
 */
export interface SetupTask {
  /**
   * The command line to be executed on each cluster's node after it being allocated or rebooted. The command is executed in a bash subshell as a root.
   */
  commandLine: string;
  /**
   * A collection of user defined environment variables to be set for setup task.
   */
  environmentVariables?: EnvironmentVariable[];
  /**
   * A collection of user defined environment variables with secret values to be set for the setup task. Server will never report values of these variables back.
   */
  secrets?: EnvironmentVariableWithSecretValue[];
  /**
   * The prefix of a path where the Batch AI service will upload the stdout, stderr and execution log of the setup task.
   */
  stdOutErrPathPrefix: string;
  /**
   * A path segment appended by Batch AI to stdOutErrPathPrefix to form a path where stdout, stderr and execution log of the setup task will be uploaded. Batch AI creates the setup task output directories under an unique path to avoid conflicts between different clusters. The full path can be obtained by concatenation of stdOutErrPathPrefix and stdOutErrPathSuffix.
   */
  readonly stdOutErrPathSuffix?: string;
}

/**
 * Performance counters reporting settings.
 */
export interface PerformanceCountersSettings {
  /**
   * Azure Application Insights information for performance counters reporting. If provided, Batch AI will upload node performance counters to the corresponding Azure Application Insights account.
   */
  appInsightsReference: AppInsightsReference;
}

/**
 * Azure Application Insights information for performance counters reporting.
 */
export interface AppInsightsReference {
  /**
   * Azure Application Insights component resource ID.
   */
  component: ResourceId;
  /**
   * Value of the Azure Application Insights instrumentation key.
   */
  instrumentationKey?: string;
  /**
   * KeyVault Store and Secret which contains Azure Application Insights instrumentation key. One of instrumentationKey or instrumentationKeySecretReference must be specified.
   */
  instrumentationKeySecretReference?: KeyVaultSecretReference;
}

/**
 * Information about a Cluster.
 */
export type Cluster = ProxyResource & {
  /**
   * The size of the virtual machines in the cluster. All nodes in a cluster have the same VM size.
   */
  vmSize?: string;
  /**
   * VM priority of cluster nodes.
   */
  vmPriority?: VmPriority;
  /**
   * Scale settings of the cluster.
   */
  scaleSettings?: ScaleSettings;
  /**
   * Virtual machine configuration (OS image) of the compute nodes. All nodes in a cluster have the same OS image configuration.
   */
  virtualMachineConfiguration?: VirtualMachineConfiguration;
  /**
   * Setup (mount file systems, performance counters settings and custom setup task) to be performed on each compute node in the cluster.
   */
  nodeSetup?: NodeSetup;
  /**
   * Administrator user account settings which can be used to SSH to compute nodes.
   */
  userAccountSettings?: UserAccountSettings;
  /**
   * Virtual network subnet resource ID the cluster nodes belong to.
   */
  subnet?: ResourceId;
  /**
   * The time when the cluster was created.
   */
  readonly creationTime?: Date;
  /**
   * Provisioning state of the cluster. Possible value are: creating - Specifies that the cluster is being created. succeeded - Specifies that the cluster has been created successfully. failed - Specifies that the cluster creation has failed. deleting - Specifies that the cluster is being deleted.
   */
  readonly provisioningState?: ProvisioningState;
  /**
   * Time when the provisioning state was changed.
   */
  readonly provisioningStateTransitionTime?: Date;
  /**
   * Allocation state of the cluster. Possible values are: steady - Indicates that the cluster is not resizing. There are no changes to the number of compute nodes in the cluster in progress. A cluster enters this state when it is created and when no operations are being performed on the cluster to change the number of compute nodes. resizing - Indicates that the cluster is resizing; that is, compute nodes are being added to or removed from the cluster.
   */
  readonly allocationState?: AllocationState;
  /**
   * The time at which the cluster entered its current allocation state.
   */
  readonly allocationStateTransitionTime?: Date;
  /**
   * Collection of errors encountered by various compute nodes during node setup.
   */
  readonly errors?: BatchAIError[];
  /**
   * The number of compute nodes currently assigned to the cluster.
   */
  readonly currentNodeCount?: number;
  /**
   * Counts of various node states on the cluster.
   */
  readonly nodeStateCounts?: NodeStateCounts;
};

/**
 * Counts of various compute node states on the cluster.
 */
export interface NodeStateCounts {
  /**
   * Number of compute nodes in idle state.
   */
  readonly idleNodeCount?: number;
  /**
   * Number of compute nodes which are running jobs.
   */
  readonly runningNodeCount?: number;
  /**
   * Number of compute nodes which are being prepared.
   */
  readonly preparingNodeCount?: number;
  /**
   * Number of compute nodes which are in unusable state.
   */
  readonly unusableNodeCount?: number;
  /**
   * Number of compute nodes which are leaving the cluster.
   */
  readonly leavingNodeCount?: number;
}

/**
 * Cluster update parameters.
 */
export interface ClusterUpdateParameters {
  /**
   * Desired scale settings for the cluster. Batch AI service supports manual and auto scale clusters.
   */
  scaleSettings?: ScaleSettings;
}

/**
 * Values returned by the List Clusters operation.
 */
export interface ClusterListResult {
  /**
   * The collection of returned Clusters.
   */
  readonly value?: Cluster[];
  /**
   * The continuation token.
   */
  readonly nextLink?: string;
}

/**
 * Parameter group
 */
export interface WorkspacesListOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface WorkspacesListByResourceGroupOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface ExperimentsListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface JobsListByExperimentOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface JobsListOutputFilesOptions {
  /**
   * Id of the job output directory. This is the OutputDirectory-->id parameter that is given by the user during Create Job.
   */
  outputdirectoryid: string;
  /**
   * The path to the directory.
   */
  directory?: string;
  /**
   * The number of minutes after which the download link will expire.
   */
  linkexpiryinminutes?: number;
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface FileServersListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Parameter group
 */
export interface ClustersListByWorkspaceOptions {
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Defines values for ProvisioningState.
 */
export type ProvisioningState =
  | "creating"
  | "succeeded"
  | "failed"
  | "deleting";
/**
 * Defines values for JobPriority.
 */
export type JobPriority = "low" | "normal" | "high";
/**
 * Defines values for ToolType.
 */
export type ToolType =
  | "cntk"
  | "tensorflow"
  | "caffe"
  | "caffe2"
  | "chainer"
  | "horovod"
  | "custommpi"
  | "custom";
/**
 * Defines values for ExecutionState.
 */
export type ExecutionState =
  | "queued"
  | "running"
  | "terminating"
  | "succeeded"
  | "failed";
/**
 * Defines values for FileType.
 */
export type FileType = "file" | "directory";
/**
 * Defines values for StorageAccountType.
 */
export type StorageAccountType = "Standard_LRS" | "Premium_LRS";
/**
 * Defines values for FileServerProvisioningState.
 */
export type FileServerProvisioningState =
  | "creating"
  | "updating"
  | "deleting"
  | "succeeded"
  | "failed";
/**
 * Defines values for DeallocationOption.
 */
export type DeallocationOption =
  | "requeue"
  | "terminate"
  | "waitforjobcompletion";
/**
 * Defines values for AllocationState.
 */
export type AllocationState = "steady" | "resizing";
/**
 * Defines values for CachingType.
 */
export type CachingType = "none" | "readonly" | "readwrite";
/**
 * Defines values for VmPriority.
 */
export type VmPriority = "dedicated" | "lowpriority";

/**
 * Contains response data for the list operation.
 */
export type OperationsListResponse = OperationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: OperationListResult;
  };
};

/**
 * Contains response data for the listNext operation.
 */
export type OperationsListNextResponse = OperationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: OperationListResult;
  };
};

/**
 * Contains response data for the list operation.
 */
export type UsagesListResponse = ListUsagesResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ListUsagesResult;
  };
};

/**
 * Contains response data for the listNext operation.
 */
export type UsagesListNextResponse = ListUsagesResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ListUsagesResult;
  };
};

/**
 * Optional parameters.
 */
export interface WorkspacesListOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  workspacesListOptions?: WorkspacesListOptions;
}

/**
 * Contains response data for the list operation.
 */
export type WorkspacesListResponse = WorkspaceListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: WorkspaceListResult;
  };
};

/**
 * Optional parameters.
 */
export interface WorkspacesListByResourceGroupOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  workspacesListByResourceGroupOptions?: WorkspacesListByResourceGroupOptions;
}

/**
 * Contains response data for the listByResourceGroup operation.
 */
export type WorkspacesListByResourceGroupResponse = WorkspaceListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: WorkspaceListResult;
  };
};

/**
 * Contains response data for the create operation.
 */
export type WorkspacesCreateResponse = Workspace & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Workspace;
  };
};

/**
 * Contains response data for the update operation.
 */
export type WorkspacesUpdateResponse = Workspace & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Workspace;
  };
};

/**
 * Contains response data for the getModel operation.
 */
export type WorkspacesGetResponse = Workspace & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Workspace;
  };
};

/**
 * Optional parameters.
 */
export interface WorkspacesListNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  workspacesListOptions?: WorkspacesListOptions;
}

/**
 * Contains response data for the listNext operation.
 */
export type WorkspacesListNextResponse = WorkspaceListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: WorkspaceListResult;
  };
};

/**
 * Optional parameters.
 */
export interface WorkspacesListByResourceGroupNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  workspacesListByResourceGroupOptions?: WorkspacesListByResourceGroupOptions;
}

/**
 * Contains response data for the listByResourceGroupNext operation.
 */
export type WorkspacesListByResourceGroupNextResponse = WorkspaceListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: WorkspaceListResult;
  };
};

/**
 * Optional parameters.
 */
export interface ExperimentsListByWorkspaceOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  experimentsListByWorkspaceOptions?: ExperimentsListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspace operation.
 */
export type ExperimentsListByWorkspaceResponse = ExperimentListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ExperimentListResult;
  };
};

/**
 * Contains response data for the create operation.
 */
export type ExperimentsCreateResponse = Experiment & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Experiment;
  };
};

/**
 * Contains response data for the getModel operation.
 */
export type ExperimentsGetResponse = Experiment & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Experiment;
  };
};

/**
 * Optional parameters.
 */
export interface ExperimentsListByWorkspaceNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  experimentsListByWorkspaceOptions?: ExperimentsListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspaceNext operation.
 */
export type ExperimentsListByWorkspaceNextResponse = ExperimentListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ExperimentListResult;
  };
};

/**
 * Optional parameters.
 */
export interface JobsListByExperimentOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  jobsListByExperimentOptions?: JobsListByExperimentOptions;
}

/**
 * Contains response data for the listByExperiment operation.
 */
export type JobsListByExperimentResponse = JobListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: JobListResult;
  };
};

/**
 * Contains response data for the create operation.
 */
export type JobsCreateResponse = Job & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Job;
  };
};

/**
 * Contains response data for the getModel operation.
 */
export type JobsGetResponse = Job & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Job;
  };
};

/**
 * Optional parameters.
 */
export interface JobsListOutputFilesOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * The path to the directory.
   */
  directory?: string;
  /**
   * The number of minutes after which the download link will expire.
   */
  linkexpiryinminutes?: number;
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Contains response data for the listOutputFiles operation.
 */
export type JobsListOutputFilesResponse = FileListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileListResult;
  };
};

/**
 * Contains response data for the listRemoteLoginInformation operation.
 */
export type JobsListRemoteLoginInformationResponse = RemoteLoginInformationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: RemoteLoginInformationListResult;
  };
};

/**
 * Optional parameters.
 */
export interface JobsListByExperimentNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  jobsListByExperimentOptions?: JobsListByExperimentOptions;
}

/**
 * Contains response data for the listByExperimentNext operation.
 */
export type JobsListByExperimentNextResponse = JobListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: JobListResult;
  };
};

/**
 * Optional parameters.
 */
export interface JobsListOutputFilesNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * The path to the directory.
   */
  directory?: string;
  /**
   * The number of minutes after which the download link will expire.
   */
  linkexpiryinminutes?: number;
  /**
   * The maximum number of items to return in the response. A maximum of 1000 files can be returned.
   */
  maxResults?: number;
}

/**
 * Contains response data for the listOutputFilesNext operation.
 */
export type JobsListOutputFilesNextResponse = FileListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileListResult;
  };
};

/**
 * Contains response data for the listRemoteLoginInformationNext operation.
 */
export type JobsListRemoteLoginInformationNextResponse = RemoteLoginInformationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: RemoteLoginInformationListResult;
  };
};

/**
 * Contains response data for the create operation.
 */
export type FileServersCreateResponse = FileServer & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileServer;
  };
};

/**
 * Contains response data for the getModel operation.
 */
export type FileServersGetResponse = FileServer & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileServer;
  };
};

/**
 * Optional parameters.
 */
export interface FileServersListByWorkspaceOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  fileServersListByWorkspaceOptions?: FileServersListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspace operation.
 */
export type FileServersListByWorkspaceResponse = FileServerListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileServerListResult;
  };
};

/**
 * Optional parameters.
 */
export interface FileServersListByWorkspaceNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  fileServersListByWorkspaceOptions?: FileServersListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspaceNext operation.
 */
export type FileServersListByWorkspaceNextResponse = FileServerListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: FileServerListResult;
  };
};

/**
 * Contains response data for the create operation.
 */
export type ClustersCreateResponse = Cluster & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Cluster;
  };
};

/**
 * Contains response data for the update operation.
 */
export type ClustersUpdateResponse = Cluster & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Cluster;
  };
};

/**
 * Contains response data for the getModel operation.
 */
export type ClustersGetResponse = Cluster & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: Cluster;
  };
};

/**
 * Contains response data for the listRemoteLoginInformation operation.
 */
export type ClustersListRemoteLoginInformationResponse = RemoteLoginInformationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: RemoteLoginInformationListResult;
  };
};

/**
 * Optional parameters.
 */
export interface ClustersListByWorkspaceOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  clustersListByWorkspaceOptions?: ClustersListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspace operation.
 */
export type ClustersListByWorkspaceResponse = ClusterListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ClusterListResult;
  };
};

/**
 * Contains response data for the listRemoteLoginInformationNext operation.
 */
export type ClustersListRemoteLoginInformationNextResponse = RemoteLoginInformationListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: RemoteLoginInformationListResult;
  };
};

/**
 * Optional parameters.
 */
export interface ClustersListByWorkspaceNextOptionalParams
  extends coreHttp.OperationOptions {
  /**
   * Parameter group
   */
  clustersListByWorkspaceOptions?: ClustersListByWorkspaceOptions;
}

/**
 * Contains response data for the listByWorkspaceNext operation.
 */
export type ClustersListByWorkspaceNextResponse = ClusterListResult & {
  /**
   * The underlying HTTP response.
   */
  _response: coreHttp.HttpResponse & {
    /**
     * The response body as text (string format)
     */
    bodyAsText: string;

    /**
     * The response body as parsed JSON or XML
     */
    parsedBody: ClusterListResult;
  };
};

/**
 * Optional parameters.
 */
export interface BatchAIOptionalParams extends coreHttp.ServiceClientOptions {
  /**
   * server parameter
   */
  $host?: string;
  /**
   * Api Version
   */
  apiVersion?: string;
  /**
   * Overrides client endpoint.
   */
  endpoint?: string;
}
