{"version":3,"file":"models.js","sourceRoot":"","sources":["../../../src/models/models.ts"],"names":[],"mappings":"AAAA,kCAAkC","sourcesContent":["// Licensed under the MIT license.\n\nexport interface CreateModerationRequest {\n  /** The input text to classify */\n  input: string | string[];\n  /**\n   * Two content moderations models are available: `text-moderation-stable` and\n   * `text-moderation-latest`. The default is `text-moderation-latest` which will be automatically\n   * upgraded over time. This ensures you are always using our most accurate model. If you use\n   * `text-moderation-stable`, we will provide advanced notice before updating the model. Accuracy\n   * of `text-moderation-stable` may be slightly lower than for `text-moderation-latest`.\n   */\n  model?: string | \"text-moderation-latest\" | \"text-moderation-stable\";\n}\n\nexport interface CreateModerationResponse {\n  /** The unique identifier for the moderation request. */\n  id: string;\n  /** The model used to generate the moderation results. */\n  model: string;\n  /** A list of moderation objects. */\n  results: {\n    flagged: boolean;\n    categories: {\n      hate: boolean;\n      \"hate/threatening\": boolean;\n      harassment: boolean;\n      \"harassment/threatening\": boolean;\n      selfHarm: boolean;\n      \"selfHarm/intent\": boolean;\n      \"selfHarm/instructive\": boolean;\n      sexual: boolean;\n      \"sexual/minors\": boolean;\n      violence: boolean;\n      \"violence/graphic\": boolean;\n    };\n    categoryScores: {\n      hate: number;\n      \"hate/threatening\": number;\n      harassment: number;\n      \"harassment/threatening\": number;\n      selfHarm: number;\n      \"selfHarm/intent\": number;\n      \"selfHarm/instructive\": number;\n      sexual: number;\n      \"sexual/minors\": number;\n      violence: number;\n      \"violence/graphic\": number;\n    };\n  }[];\n}\n\nexport interface Error {\n  type: string;\n  message: string;\n  param: string | null;\n  code: string | null;\n}\n\nexport interface CreateImageRequest {\n  /** A text description of the desired image(s). The maximum length is 1000 characters. */\n  prompt: string;\n  /** The number of images to generate. Must be between 1 and 10. */\n  n?: number | null;\n  /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\";\n  /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */\n  responseFormat?: \"url\" | \"b64_json\";\n  user?: string;\n}\n\nexport interface ImagesResponse {\n  created: Date;\n  data: Image[];\n}\n\n/** Represents the url or the content of an image generated by the OpenAI API. */\nexport interface Image {\n  /** The URL of the generated image, if `response_format` is `url` (default). */\n  url?: string;\n  /** The base64-encoded JSON of the generated image, if `response_format` is `b64_json`. */\n  b64Json?: Uint8Array;\n}\n\nexport interface CreateImageEditRequest {\n  /** A text description of the desired image(s). The maximum length is 1000 characters. */\n  prompt: string;\n  /**\n   * The image to edit. Must be a valid PNG file, less than 4MB, and square. If mask is not\n   * provided, image must have transparency, which will be used as the mask.\n   */\n  image: Uint8Array;\n  /**\n   * An additional image whose fully transparent areas (e.g. where alpha is zero) indicate where\n   * `image` should be edited. Must be a valid PNG file, less than 4MB, and have the same dimensions\n   * as `image`.\n   */\n  mask?: Uint8Array;\n  /** The number of images to generate. Must be between 1 and 10. */\n  n?: number | null;\n  /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\";\n  /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */\n  responseFormat?: \"url\" | \"b64_json\";\n  user?: string;\n}\n\nexport interface CreateImageVariationRequest {\n  /**\n   * The image to use as the basis for the variation(s). Must be a valid PNG file, less than 4MB,\n   * and square.\n   */\n  image: Uint8Array;\n  /** The number of images to generate. Must be between 1 and 10. */\n  n?: number | null;\n  /** The size of the generated images. Must be one of `256x256`, `512x512`, or `1024x1024`. */\n  size?: \"256x256\" | \"512x512\" | \"1024x1024\";\n  /** The format in which the generated images are returned. Must be one of `url` or `b64_json`. */\n  responseFormat?: \"url\" | \"b64_json\";\n  user?: string;\n}\n\nexport interface ListModelsResponse {\n  object: string;\n  data: Model[];\n}\n\n/** Describes an OpenAI model offering that can be used with the API. */\nexport interface Model {\n  /** The model identifier, which can be referenced in the API endpoints. */\n  id: string;\n  /** The object type, which is always \"model\". */\n  object: \"model\";\n  /** The Unix timestamp (in seconds) when the model was created. */\n  created: Date;\n  /** The organization that owns the model. */\n  ownedBy: string;\n}\n\nexport interface DeleteModelResponse {\n  id: string;\n  object: string;\n  deleted: boolean;\n}\n\nexport interface CreateFineTuneRequest {\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](/docs/api-reference/files/upload) for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file, where each training example is a JSON object\n   * with the keys \"prompt\" and \"completion\". Additionally, you must upload your file with the\n   * purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more\n   * details.\n   */\n  trainingFile: string;\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics periodically during\n   * fine-tuning. These metrics can be viewed in the\n   * [fine-tuning results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).\n   * Your train and validation data should be mutually exclusive.\n   *\n   * Your dataset must be formatted as a JSONL file, where each validation example is a JSON object\n   * with the keys \"prompt\" and \"completion\". Additionally, you must upload your file with the\n   * purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/legacy-fine-tuning/creating-training-data) for more\n   * details.\n   */\n  validationFile?: string | null;\n  /**\n   * The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\", \"curie\",\n   * \"davinci\", or a fine-tuned model created after 2022-04-21 and before 2023-08-22. To learn more\n   * about these models, see the [Models](/docs/models) documentation.\n   */\n  model?: string | \"ada\" | \"babbage\" | \"curie\" | \"davinci\";\n  /**\n   * The number of epochs to train the model for. An epoch refers to one full cycle through the\n   * training dataset.\n   */\n  nEpochs?: number | null;\n  /**\n   * The batch size to use for training. The batch size is the number of training examples used to\n   * train a single forward and backward pass.\n   *\n   * By default, the batch size will be dynamically configured to be ~0.2% of the number of examples\n   * in the training set, capped at 256 - in general, we've found that larger batch sizes tend to\n   * work better for larger datasets.\n   */\n  batchSize?: number | null;\n  /**\n   * The learning rate multiplier to use for training. The fine-tuning learning rate is the original\n   * learning rate used for pretraining multiplied by this value.\n   *\n   * By default, the learning rate multiplier is the 0.05, 0.1, or 0.2 depending on final\n   * `batch_size` (larger learning rates tend to perform better with larger batch sizes). We\n   * recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best\n   * results.\n   */\n  learningRateMultiplier?: number | null;\n  /**\n   * The weight to use for loss on the prompt tokens. This controls how much the model tries to\n   * learn to generate the prompt (as compared to the completion which always has a weight of 1.0),\n   * and can add a stabilizing effect to training when completions are short.\n   *\n   * If prompts are extremely long (relative to completions), it may make sense to reduce this\n   * weight so as to avoid over-prioritizing learning the prompt.\n   */\n  promptLossRate?: number | null;\n  /**\n   * If set, we calculate classification-specific metrics such as accuracy and F-1 score using the\n   * validation set at the end of every epoch. These metrics can be viewed in the\n   * [results file](/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).\n   *\n   * In order to compute classification metrics, you must provide a `validation_file`. Additionally,\n   * you must specify `classification_n_classes` for multiclass classification or\n   * `classification_positive_class` for binary classification.\n   */\n  computeClassificationMetrics?: boolean | null;\n  /**\n   * The number of classes in a classification task.\n   *\n   * This parameter is required for multiclass classification.\n   */\n  classificationNClasses?: number | null;\n  /**\n   * The positive class in binary classification.\n   *\n   * This parameter is needed to generate precision, recall, and F1 metrics when doing binary\n   * classification.\n   */\n  classificationPositiveClass?: string | null;\n  /**\n   * If this is provided, we calculate F-beta scores at the specified beta values. The F-beta score\n   * is a generalization of F-1 score. This is only used for binary classification.\n   *\n   * With a beta of 1 (i.e. the F-1 score), precision and recall are given the same weight. A larger\n   * beta score puts more weight on recall and less on precision. A smaller beta score puts more\n   * weight on precision and less on recall.\n   */\n  classificationBetas?: number[] | null;\n  /**\n   * A string of up to 18 characters that will be added to your fine-tuned model name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.\n   */\n  suffix?: string | null;\n}\n\n/** The `FineTune` object represents a legacy fine-tune job that has been created through the API. */\nexport interface FineTune {\n  /** The object identifier, which can be referenced in the API endpoints. */\n  id: string;\n  /** The object type, which is always \"fine-tune\". */\n  object: \"fine-tune\";\n  /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */\n  createdAt: Date;\n  /** The Unix timestamp (in seconds) for when the fine-tuning job was last updated. */\n  updatedAt: Date;\n  /** The base model that is being fine-tuned. */\n  model: string;\n  /** The name of the fine-tuned model that is being created. */\n  fineTunedModel: string | null;\n  /** The organization that owns the fine-tuning job. */\n  organizationId: string;\n  /**\n   * The current status of the fine-tuning job, which can be either `created`, `running`,\n   * `succeeded`, `failed`, or `cancelled`.\n   */\n  status: \"created\" | \"running\" | \"succeeded\" | \"failed\" | \"cancelled\";\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](/docs/guides/legacy-fine-tuning/hyperparameters) for more details.\n   */\n  hyperparams: {\n    nEpochs: number;\n    batchSize: number;\n    promptLossWeight: number;\n    learningRateMultiplier: number;\n    computeClassificationMetrics?: boolean;\n    classificationPositiveClass?: string;\n    classificationNClasses?: number;\n  };\n  /** The list of files used for training. */\n  trainingFiles: OpenAIFile[];\n  /** The list of files used for validation. */\n  validationFiles: OpenAIFile[];\n  /** The compiled results files for the fine-tuning job. */\n  resultFiles: OpenAIFile[];\n  /** The list of events that have been observed in the lifecycle of the FineTune job. */\n  events?: FineTuneEvent[];\n}\n\n/** The `File` object represents a document that has been uploaded to OpenAI. */\nexport interface OpenAIFile {\n  /** The file identifier, which can be referenced in the API endpoints. */\n  id: string;\n  /** The object type, which is always \"file\". */\n  object: \"file\";\n  /** The size of the file in bytes. */\n  bytes: number;\n  /** The Unix timestamp (in seconds) for when the file was created. */\n  createdAt: Date;\n  /** The name of the file. */\n  filename: string;\n  /** The intended purpose of the file. Currently, only \"fine-tune\" is supported. */\n  purpose: string;\n  /**\n   * The current status of the file, which can be either `uploaded`, `processed`, `pending`,\n   * `error`, `deleting` or `deleted`.\n   */\n  status:\n    | \"uploaded\"\n    | \"processed\"\n    | \"pending\"\n    | \"error\"\n    | \"deleting\"\n    | \"deleted\";\n  /**\n   * Additional details about the status of the file. If the file is in the `error` state, this will\n   * include a message describing the error.\n   */\n  statusDetails?: string | null;\n}\n\nexport interface FineTuneEvent {\n  object: string;\n  createdAt: Date;\n  level: string;\n  message: string;\n}\n\nexport interface ListFineTunesResponse {\n  object: string;\n  data: FineTune[];\n}\n\nexport interface ListFineTuneEventsResponse {\n  object: string;\n  data: FineTuneEvent[];\n}\n\nexport interface ListFilesResponse {\n  object: string;\n  data: OpenAIFile[];\n}\n\nexport interface CreateFileRequest {\n  /**\n   * Name of the [JSON Lines](https://jsonlines.readthedocs.io/en/latest/) file to be uploaded.\n   *\n   * If the `purpose` is set to \"fine-tune\", the file will be used for fine-tuning.\n   */\n  file: Uint8Array;\n  /**\n   * The intended purpose of the uploaded documents. Use \"fine-tune\" for\n   * [fine-tuning](/docs/api-reference/fine-tuning). This allows us to validate the format of the\n   * uploaded file.\n   */\n  purpose: string;\n}\n\nexport interface DeleteFileResponse {\n  id: string;\n  object: string;\n  deleted: boolean;\n}\n\nexport interface CreateEmbeddingRequest {\n  /** ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them. */\n  model: string | \"text-embedding-ada-002\";\n  /**\n   * Input text to embed, encoded as a string or array of tokens. To embed multiple inputs in a\n   * single request, pass an array of strings or array of token arrays. Each input must not exceed\n   * the max input tokens for the model (8191 tokens for `text-embedding-ada-002`) and cannot be an empty string.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  input: string | string[] | number[] | number[][];\n  user?: string;\n}\n\nexport interface CreateEmbeddingResponse {\n  /** The object type, which is always \"embedding\". */\n  object: \"embedding\";\n  /** The name of the model used to generate the embedding. */\n  model: string;\n  /** The list of embeddings generated by the model. */\n  data: Embedding[];\n  /** The usage information for the request. */\n  usage: { promptTokens: number; totalTokens: number };\n}\n\n/** Represents an embedding vector returned by embedding endpoint. */\nexport interface Embedding {\n  /** The index of the embedding in the list of embeddings. */\n  index: number;\n  /** The object type, which is always \"embedding\". */\n  object: \"embedding\";\n  /**\n   * The embedding vector, which is a list of floats. The length of vector depends on the model as\\\n   * listed in the [embedding guide](/docs/guides/embeddings).\n   */\n  embedding: number[];\n}\n\nexport interface CreateEditRequest {\n  /**\n   * ID of the model to use. You can use the `text-davinci-edit-001` or `code-davinci-edit-001`\n   * model with this endpoint.\n   */\n  model: string | \"text-davinci-edit-001\" | \"code-davinci-edit-001\";\n  /** The input text to use as a starting point for the edit. */\n  input?: string | null;\n  /** The instruction that tells the model how to edit the prompt. */\n  instruction: string;\n  /** How many edits to generate for the input and instruction. */\n  n?: number | null;\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output\n   * more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers\n   * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising\n   * the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  topP?: number | null;\n}\n\nexport interface CreateEditResponse {\n  /** The object type, which is always `edit`. */\n  object: \"edit\";\n  /** The Unix timestamp (in seconds) of when the edit was created. */\n  created: Date;\n  /** description: A list of edit choices. Can be more than one if `n` is greater than 1. */\n  choices: { text: string; index: number; finishReason: \"stop\" | \"length\" }[];\n  usage: CompletionUsage;\n}\n\n/** Usage statistics for the completion request. */\nexport interface CompletionUsage {\n  /** Number of tokens in the prompt. */\n  promptTokens: number;\n  /** Number of tokens in the generated completion */\n  completionTokens: number;\n  /** Total number of tokens used in the request (prompt + completion). */\n  totalTokens: number;\n}\n\nexport interface CreateCompletionRequest {\n  /**\n   * ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to\n   * see all of your available models, or see our [Model overview](/docs/models/overview) for\n   * descriptions of them.\n   */\n  model:\n    | string\n    | \"babbage-002\"\n    | \"davinci-002\"\n    | \"text-davinci-003\"\n    | \"text-davinci-002\"\n    | \"text-davinci-001\"\n    | \"code-davinci-002\"\n    | \"text-curie-001\"\n    | \"text-babbage-001\"\n    | \"text-ada-001\";\n  /**\n   * The prompt(s) to generate completions for, encoded as a string, array of strings, array of\n   * tokens, or array of token arrays.\n   *\n   * Note that <|endoftext|> is the document separator that the model sees during training, so if a\n   * prompt is not specified the model will generate as if from the beginning of a new document.\n   */\n  prompt: Prompt;\n  /** The suffix that comes after a completion of inserted text. */\n  suffix?: string | null;\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output\n   * more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers\n   * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising\n   * the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  topP?: number | null;\n  /**\n   * How many completions to generate for each prompt.\n   * **Note:** Because this parameter generates many completions, it can quickly consume your token\n   * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n   */\n  n?: number | null;\n  /**\n   * The maximum number of [tokens](/tokenizer) to generate in the completion.\n   *\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  maxTokens?: number | null;\n  /** Up to 4 sequences where the API will stop generating further tokens. */\n  stop?: Stop;\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear\n   * in the text so far, increasing the model's likelihood to talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n   */\n  presencePenalty?: number | null;\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing\n   * frequency in the text so far, decreasing the model's likelihood to repeat the same line\n   * verbatim.\n   *\n   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n   */\n  frequencyPenalty?: number | null;\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an\n   * associated bias value from -100 to 100. Mathematically, the bias is added to the logits\n   * generated by the model prior to sampling. The exact effect will vary per model, but values\n   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100\n   * should result in a ban or exclusive selection of the relevant token.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect\n   * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]` message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: boolean | null;\n  /**\n   * Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens.\n   * For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The\n   * API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1`\n   * elements in the response.\n   *\n   * The maximum value for `logprobs` is 5.\n   */\n  logprobs?: number | null;\n  /** Echo back the prompt in addition to the completion */\n  echo?: boolean | null;\n  /**\n   * Generates `best_of` completions server-side and returns the \"best\" (the one with the highest\n   * log probability per token). Results cannot be streamed.\n   *\n   * When used with `n`, `best_of` controls the number of candidate completions and `n` specifies\n   * how many to return â€“ `best_of` must be greater than `n`.\n   *\n   * **Note:** Because this parameter generates many completions, it can quickly consume your token\n   * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n   */\n  bestOf?: number | null;\n}\n\n/**\n * Represents a completion response from the API. Note: both the streamed and non-streamed response\n * objects share the same shape (unlike the chat endpoint).\n */\nexport interface CreateCompletionResponse {\n  /** A unique identifier for the completion. */\n  id: string;\n  /** The object type, which is always `text_completion`. */\n  object: string;\n  /** The Unix timestamp (in seconds) of when the completion was created. */\n  created: Date;\n  /** The model used for the completion. */\n  model: string;\n  /** The list of completion choices the model generated for the input. */\n  choices: {\n    index: number;\n    text: string;\n    logprobs: {\n      tokens: string[];\n      tokenLogprobs: number[];\n      topLogprobs: Record<string, number>[];\n      textOffset: number[];\n    } | null;\n    finishReason: \"stop\" | \"length\" | \"content_filter\";\n  }[];\n  usage?: CompletionUsage;\n}\n\nexport interface CreateFineTuningJobRequest {\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](/docs/api-reference/files/upload) for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file. Additionally, you must upload your file with\n   * the purpose `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.\n   */\n  trainingFile: string;\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics periodically during\n   * fine-tuning. These metrics can be viewed in the fine-tuning results file. The same data should\n   * not be present in both train and validation files.\n   *\n   * Your dataset must be formatted as a JSONL file. You must upload your file with the purpose\n   * `fine-tune`.\n   *\n   * See the [fine-tuning guide](/docs/guides/fine-tuning) for more details.\n   */\n  validationFile?: string | null;\n  /**\n   * The name of the model to fine-tune. You can select one of the\n   * [supported models](/docs/guides/fine-tuning/what-models-can-be-fine-tuned).\n   */\n  model: string | \"babbage-002\" | \"davinci-002\" | \"gpt-3.5-turbo\";\n  /** The hyperparameters used for the fine-tuning job. */\n  hyperparameters?: { nEpochs?: \"auto\" | number };\n  /**\n   * A string of up to 18 characters that will be added to your fine-tuned model name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ft:gpt-3.5-turbo:openai:custom-model-name:7p4lURel`.\n   */\n  suffix?: string | null;\n}\n\nexport interface FineTuningJob {\n  /** The object identifier, which can be referenced in the API endpoints. */\n  id: string;\n  /** The object type, which is always \"fine_tuning.job\". */\n  object: \"fine_tuning.job\";\n  /** The Unix timestamp (in seconds) for when the fine-tuning job was created. */\n  createdAt: Date;\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was finished. The value will be\n   * null if the fine-tuning job is still running.\n   */\n  finishedAt: Date | null;\n  /** The base model that is being fine-tuned. */\n  model: string;\n  /**\n   * The name of the fine-tuned model that is being created. The value will be null if the\n   * fine-tuning job is still running.\n   */\n  fineTunedModel: string | null;\n  /** The organization that owns the fine-tuning job. */\n  organizationId: string;\n  /**\n   * The current status of the fine-tuning job, which can be either `created`, `pending`, `running`,\n   * `succeeded`, `failed`, or `cancelled`.\n   */\n  status:\n    | \"created\"\n    | \"pending\"\n    | \"running\"\n    | \"succeeded\"\n    | \"failed\"\n    | \"cancelled\";\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](/docs/guides/fine-tuning) for more details.\n   */\n  hyperparameters: { nEpochs?: \"auto\" | number };\n  /**\n   * The file ID used for training. You can retrieve the training data with the\n   * [Files API](/docs/api-reference/files/retrieve-contents).\n   */\n  trainingFile: string;\n  /**\n   * The file ID used for validation. You can retrieve the validation results with the\n   * [Files API](/docs/api-reference/files/retrieve-contents).\n   */\n  validationFile: string | null;\n  /**\n   * The compiled results file ID(s) for the fine-tuning job. You can retrieve the results with the\n   * [Files API](/docs/api-reference/files/retrieve-contents).\n   */\n  resultFiles: string[];\n  /**\n   * The total number of billable tokens processed by this fine tuning job. The value will be null\n   * if the fine-tuning job is still running.\n   */\n  trainedTokens: number | null;\n  /**\n   * For fine-tuning jobs that have `failed`, this will contain more information on the cause of the\n   * failure.\n   */\n  error: { message?: string; code?: string; param?: string | null } | null;\n}\n\nexport interface ListPaginatedFineTuningJobsResponse {\n  object: string;\n  data: FineTuningJob[];\n  hasMore: boolean;\n}\n\nexport interface ListFineTuningJobEventsResponse {\n  object: string;\n  data: FineTuningJobEvent[];\n}\n\nexport interface FineTuningJobEvent {\n  id: string;\n  object: string;\n  createdAt: Date;\n  level: \"info\" | \"warn\" | \"error\";\n  message: string;\n}\n\nexport interface CreateChatCompletionRequest {\n  /**\n   * ID of the model to use. See the [model endpoint compatibility](/docs/models/model-endpoint-compatibility)\n   * table for details on which models work with the Chat API.\n   */\n  model:\n    | string\n    | \"gpt4\"\n    | \"gpt-4-0314\"\n    | \"gpt-4-0613\"\n    | \"gpt-4-32k\"\n    | \"gpt-4-32k-0314\"\n    | \"gpt-4-32k-0613\"\n    | \"gpt-3.5-turbo\"\n    | \"gpt-3.5-turbo-16k\"\n    | \"gpt-3.5-turbo-0301\"\n    | \"gpt-3.5-turbo-0613\"\n    | \"gpt-3.5-turbo-16k-0613\";\n  /**\n   * A list of messages comprising the conversation so far.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb).\n   */\n  messages: ChatCompletionRequestMessage[];\n  /** A list of functions the model may generate JSON inputs for. */\n  functions?: ChatCompletionFunctions[];\n  /**\n   * Controls how the model responds to function calls. `none` means the model does not call a\n   * function, and responds to the end-user. `auto` means the model can pick between an end-user or\n   * calling a function.  Specifying a particular function via `{\\\"name\":\\ \\\"my_function\\\"}` forces the\n   * model to call that function. `none` is the default when no functions are present. `auto` is the\n   * default if functions are present.\n   */\n  functionCall?: \"none\" | \"auto\" | ChatCompletionFunctionCallOption;\n  /**\n   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output\n   * more random, while lower values like 0.2 will make it more focused and deterministic.\n   *\n   * We generally recommend altering this or `top_p` but not both.\n   */\n  temperature?: number | null;\n  /**\n   * An alternative to sampling with temperature, called nucleus sampling, where the model considers\n   * the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising\n   * the top 10% probability mass are considered.\n   *\n   * We generally recommend altering this or `temperature` but not both.\n   */\n  topP?: number | null;\n  /**\n   * How many completions to generate for each prompt.\n   * **Note:** Because this parameter generates many completions, it can quickly consume your token\n   * quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n   */\n  n?: number | null;\n  /**\n   * The maximum number of [tokens](/tokenizer) to generate in the completion.\n   *\n   * The token count of your prompt plus `max_tokens` cannot exceed the model's context length.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)\n   * for counting tokens.\n   */\n  maxTokens?: number | null;\n  /** Up to 4 sequences where the API will stop generating further tokens. */\n  stop?: Stop;\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear\n   * in the text so far, increasing the model's likelihood to talk about new topics.\n   *\n   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n   */\n  presencePenalty?: number | null;\n  /**\n   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing\n   * frequency in the text so far, decreasing the model's likelihood to repeat the same line\n   * verbatim.\n   *\n   * [See more information about frequency and presence penalties.](/docs/guides/gpt/parameter-details)\n   */\n  frequencyPenalty?: number | null;\n  /**\n   * Modify the likelihood of specified tokens appearing in the completion.\n   * Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an\n   * associated bias value from -100 to 100. Mathematically, the bias is added to the logits\n   * generated by the model prior to sampling. The exact effect will vary per model, but values\n   * between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100\n   * should result in a ban or exclusive selection of the relevant token.\n   */\n  logitBias?: Record<string, number>;\n  /**\n   * A unique identifier representing your end-user, which can help OpenAI to monitor and detect\n   * abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n   */\n  user?: string;\n  /**\n   * If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available, with the stream terminated by a `data: [DONE]` message.\n   * [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb).\n   */\n  stream?: boolean | null;\n}\n\nexport interface ChatCompletionRequestMessage {\n  /** The role of the messages author. One of `system`, `user`, `assistant`, or `function`. */\n  role: \"system\" | \"user\" | \"assistant\" | \"function\";\n  /**\n   * The contents of the message. `content` is required for all messages, and may be null for\n   * assistant messages with function calls.\n   */\n  content: string | null;\n  /**\n   * The name of the author of this message. `name` is required if role is `function`, and it\n   * should be the name of the function whose response is in the `content`. May contain a-z,\n   * A-Z, 0-9, and underscores, with a maximum length of 64 characters.\n   */\n  name?: string;\n  /** The name and arguments of a function that should be called, as generated by the model. */\n  functionCall?: { name: string; arguments: string };\n}\n\nexport interface ChatCompletionFunctions {\n  /**\n   * The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and\n   * dashes, with a maximum length of 64.\n   */\n  name: string;\n  /**\n   * A description of what the function does, used by the model to choose when and how to call the\n   * function.\n   */\n  description?: string;\n  /**\n   * The parameters the functions accepts, described as a JSON Schema object. See the\n   * [guide](/docs/guides/gpt/function-calling) for examples, and the\n   * [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation\n   * about the format.\\n\\nTo describe a function that accepts no parameters, provide the value\n   * `{\\\"type\\\": \\\"object\\\", \\\"properties\\\": {}}`.\n   */\n  parameters: Record<string, unknown>;\n}\n\nexport interface ChatCompletionFunctionCallOption {\n  /** The name of the function to call. */\n  name: string;\n}\n\n/** Represents a chat completion response returned by model, based on the provided input. */\nexport interface CreateChatCompletionResponse {\n  /** A unique identifier for the chat completion. */\n  id: string;\n  /** The object type, which is always `chat.completion`. */\n  object: string;\n  /** The Unix timestamp (in seconds) of when the chat completion was created. */\n  created: Date;\n  /** The model used for the chat completion. */\n  model: string;\n  /** A list of chat completion choices. Can be more than one if `n` is greater than 1. */\n  choices: {\n    index: number;\n    message: ChatCompletionResponseMessage;\n    finishReason: \"stop\" | \"length\" | \"function_call\" | \"content_filter\";\n  }[];\n  usage?: CompletionUsage;\n}\n\nexport interface ChatCompletionResponseMessage {\n  /** The role of the author of this message. */\n  role: \"system\" | \"user\" | \"assistant\" | \"function\";\n  /** The contents of the message. */\n  content: string | null;\n  /** The name and arguments of a function that should be called, as generated by the model. */\n  functionCall?: { name: string; arguments: string };\n}\n\nexport interface CreateTranslationRequest {\n  /**\n   * The audio file object (not file name) to translate, in one of these formats: flac, mp3, mp4,\n   * mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uint8Array;\n  /** ID of the model to use. Only `whisper-1` is currently available. */\n  model: string | \"whisper-1\";\n  /**\n   * An optional text to guide the model's style or continue a previous audio segment. The\n   * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.\n   */\n  prompt?: string;\n  /**\n   * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or\n   * vtt.\n   */\n  responseFormat?: \"json\" | \"text\" | \"srt\" | \"verbose_json\" | \"vtt\";\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more\n   * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,\n   * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n}\n\nexport interface CreateTranslationResponse {\n  text: string;\n}\n\nexport interface CreateTranscriptionRequest {\n  /**\n   * The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4,\n   * mpeg, mpga, m4a, ogg, wav, or webm.\n   */\n  file: Uint8Array;\n  /** ID of the model to use. Only `whisper-1` is currently available. */\n  model: string | \"whisper-1\";\n  /**\n   * An optional text to guide the model's style or continue a previous audio segment. The\n   * [prompt](/docs/guides/speech-to-text/prompting) should match the audio language.\n   */\n  prompt?: string;\n  /**\n   * The format of the transcript output, in one of these options: json, text, srt, verbose_json, or\n   * vtt.\n   */\n  responseFormat?: \"json\" | \"text\" | \"srt\" | \"verbose_json\" | \"vtt\";\n  /**\n   * The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more\n   * random, while lower values like 0.2 will make it more focused and deterministic. If set to 0,\n   * the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to\n   * automatically increase the temperature until certain thresholds are hit.\n   */\n  temperature?: number;\n  /**\n   * The language of the input audio. Supplying the input language in\n   * [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy\n   * and latency.\n   */\n  language?: string;\n}\n\nexport interface CreateTranscriptionResponse {\n  text: string;\n}\n\n/** Alias for Prompt */\nexport type Prompt = string | string[] | number[] | number[][];\n/** Alias for Stop */\nexport type Stop = string | string[];\n"]}